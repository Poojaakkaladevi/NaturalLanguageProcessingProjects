{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOqYrT/VbIZsRL6ln0YDirE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"58c6f171452b4af49dc67f2bfdbc6675":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0ec909869aef4028af376a2d42387478","IPY_MODEL_5d708059058440afb1a8ffbf42dd2d82","IPY_MODEL_fbd084f96ade49cb9ec145b4c8946e1b"],"layout":"IPY_MODEL_970e893fd21d40908fe2030aa83ac797"}},"0ec909869aef4028af376a2d42387478":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_942ec39c7f684257ac40264be13fd72f","placeholder":"​","style":"IPY_MODEL_403503dd2dc14351a51ff7c55886da0d","value":"Map: 100%"}},"5d708059058440afb1a8ffbf42dd2d82":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8711ef51c2e54ea194f06b51a1cf59ac","max":500,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8ab725b7aedc492f82dc5692b68b1670","value":500}},"fbd084f96ade49cb9ec145b4c8946e1b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_61df353a586a4b94bde09efea7cf53fe","placeholder":"​","style":"IPY_MODEL_ca7967376896487da87890f47dfe4fd0","value":" 500/500 [00:00&lt;00:00, 4116.46 examples/s]"}},"970e893fd21d40908fe2030aa83ac797":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"942ec39c7f684257ac40264be13fd72f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"403503dd2dc14351a51ff7c55886da0d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8711ef51c2e54ea194f06b51a1cf59ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ab725b7aedc492f82dc5692b68b1670":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"61df353a586a4b94bde09efea7cf53fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca7967376896487da87890f47dfe4fd0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# **Machine Translation**"],"metadata":{"id":"BzbRp7VZF4MX"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PVLph3PGFiI3","executionInfo":{"status":"ok","timestamp":1698855761890,"user_tz":300,"elapsed":17,"user":{"displayName":"Phanindra Josh","userId":"16090189446352440714"}},"outputId":"3b38336e-0a2e-42eb-ca6d-b660dab2174a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Nov  1 16:22:40 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   47C    P8    12W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","source":["pip install transformers --upgrade"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L_H4E5LOdfpG","executionInfo":{"status":"ok","timestamp":1698855771058,"user_tz":300,"elapsed":9173,"user":{"displayName":"Phanindra Josh","userId":"16090189446352440714"}},"outputId":"e6b17f38-cbb5-4854-9d66-a2ee62bc9b36"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"]}]},{"cell_type":"markdown","source":["# **Setting up the Environment**"],"metadata":{"id":"eqc9Pet7GHyj"}},{"cell_type":"code","source":["from pathlib import Path\n","if 'google.colab' in str(get_ipython()):\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","    !pip install datasets transformers evaluate wandb accelerate -U -qq\n","\n","    base_folder = Path(\"/content/drive/MyDrive/Pooja_HP_Singh_Projects/NLP/HW6\")\n","\n","\n","from transformers import AutoConfig, AutoModelForSeq2SeqLM, AutoTokenizer, Seq2SeqTrainer,TrainingArguments\n","from transformers import AutoTokenizer, DataCollatorForSeq2Seq, pipeline\n","from datasets import load_dataset, DatasetDict\n","import evaluate\n","from evaluate import evaluator\n","\n","\n","import wandb\n","import numpy as np\n","import pandas as pd\n","from transformers import GenerationConfig\n","import gc\n","from transformers import Seq2SeqTrainingArguments\n","import torch\n","\n","!pip install sacrebleu\n","!pip install bert_score\n","!pip install \"transformers[sentencepiece]\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D9HSvNvgGJ96","executionInfo":{"status":"ok","timestamp":1698855828826,"user_tz":300,"elapsed":57772,"user":{"displayName":"Phanindra Josh","userId":"16090189446352440714"}},"outputId":"5f925c90-8078-46d6-c91c-274bb26d621c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.3.1)\n","Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2.8.2)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2023.6.3)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.23.5)\n","Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.3)\n","Requirement already satisfied: bert_score in /usr/local/lib/python3.10/dist-packages (0.3.13)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.1.0+cu118)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.5.3)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.34.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.31.0)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.66.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.7.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score) (23.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2023.3.post1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2.1.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.17.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (2023.6.3)\n","Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.14.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.4.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.1.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.43.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.5)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2023.7.22)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.0.1->bert_score) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n","Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.34.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.12.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.17.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2.31.0)\n","Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.14.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (4.66.1)\n","Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.1.99)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[sentencepiece]) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[sentencepiece]) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2023.7.22)\n"]}]},{"cell_type":"markdown","source":["# **Function to Load Dataset**"],"metadata":{"id":"sSwJOvatGbFr"}},{"cell_type":"code","source":["def load_dataset_from_hf(name_of_the_datacard, lang1, lang2):\n","  data = load_dataset(name_of_the_datacard, lang1=lang1, lang2=lang2)\n","  return data"],"metadata":{"id":"__by3pYGGeAm","executionInfo":{"status":"ok","timestamp":1698855828826,"user_tz":300,"elapsed":40,"user":{"displayName":"Phanindra Josh","userId":"16090189446352440714"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# **Function to Split Dataset**"],"metadata":{"id":"325KZPy6HFrC"}},{"cell_type":"code","source":["def split_dataset(data):\n","  test_val_splits = data['train'].train_test_split(test_size=0.4, seed=42)\n","  train_split= test_val_splits['train']\n","  test_val_splits = test_val_splits['test'].train_test_split(test_size=0.5, seed=42,)\n","  val_split = test_val_splits['train']\n","  test_split = test_val_splits['test']\n","  return train_split,val_split,test_split\n"],"metadata":{"id":"avfq_K93HJKz","executionInfo":{"status":"ok","timestamp":1698855828827,"user_tz":300,"elapsed":39,"user":{"displayName":"Phanindra Josh","userId":"16090189446352440714"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# **Function to Create smaller subset**"],"metadata":{"id":"RwL3nOaWHi-0"}},{"cell_type":"code","source":["def get_small_subset(train_split,val_split,test_split):\n","  # full test dataset\n","  test_dataset = test_split\n","  # combining full train and val\n","  train_val_dataset = DatasetDict({'train': train_split, 'val': val_split})\n","\n","  #creating small subsets for all splits\n","  train_split_small = train_split.shuffle(seed=42).select(range(1000))\n","  val_split_small = val_split.shuffle(seed=42).select(range(500))\n","  test_split_small = test_split.shuffle(seed=42).select(range(500))\n","  # combine train, val splits into one dataset\n","  train_val_subset = DatasetDict({'train': train_split_small, 'val': val_split_small})\n","\n","  # create test dataset from test split\n","  test_subset= DatasetDict({'test': test_split_small})\n","  return train_val_subset,train_val_dataset,test_dataset\n"],"metadata":{"id":"SOYpYFe2Hk5g","executionInfo":{"status":"ok","timestamp":1698855828827,"user_tz":300,"elapsed":37,"user":{"displayName":"Phanindra Josh","userId":"16090189446352440714"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# **Function for Tokenization**"],"metadata":{"id":"GYW1zEtpIHx-"}},{"cell_type":"code","source":["def get_tokenized_dataset(checkpoint, dataset,max_length):\n","\n","  tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","  def tokenize_fn(batch):\n","    inputs = [example['en'] for example in batch['translation']]\n","    targets = [example['fr'] for example in batch['translation']]\n","    model_inputs = tokenizer(text = inputs, text_target=targets, truncation = True, max_length=max_length)\n","    return model_inputs\n","\n","\n","  tokenized_dataset = dataset.map(tokenize_fn, batched=True, remove_columns=dataset['train'].column_names)\n","  tokenized_dataset.set_format(type=\"torch\")\n","  return tokenized_dataset"],"metadata":{"id":"MbBNvZooIKgU","executionInfo":{"status":"ok","timestamp":1698855828827,"user_tz":300,"elapsed":37,"user":{"displayName":"Phanindra Josh","userId":"16090189446352440714"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# **Function to Create Datasets**"],"metadata":{"id":"asz82T5II1kF"}},{"cell_type":"code","source":["def setup_dataset(name_of_the_datacard, lang1, lang2):\n","  #load the dataset\n","  dataset = load_dataset_from_hf(name_of_the_datacard,lang1,lang2)\n","\n","  #split the dataset\n","  train_split,val_split,test_split = split_dataset(dataset)\n","\n","  #create smaller subset\n","  train_val_subset,train_val_dataset,test_dataset = get_small_subset(train_split,val_split,test_split)\n","\n","  return train_val_subset,train_val_dataset,test_dataset"],"metadata":{"id":"BJu31pMtJN-S","executionInfo":{"status":"ok","timestamp":1698855828827,"user_tz":300,"elapsed":36,"user":{"displayName":"Phanindra Josh","userId":"16090189446352440714"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# **Function to Initialize Model**"],"metadata":{"id":"_hv4YLnMKzlj"}},{"cell_type":"code","source":["def initialize_model(checkpoint):\n","  config = AutoConfig.from_pretrained(checkpoint)\n","  generation_config = GenerationConfig.from_pretrained(checkpoint)\n","  model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint,config=config)\n","  return model\n","\n"],"metadata":{"id":"SCHuX1pqLI7C","executionInfo":{"status":"ok","timestamp":1698855828827,"user_tz":300,"elapsed":36,"user":{"displayName":"Phanindra Josh","userId":"16090189446352440714"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# **Function to Compute Metrics**"],"metadata":{"id":"DoNMQYFLL9oe"}},{"cell_type":"code","source":["bleu_metric = evaluate.load(\"sacrebleu\")\n","bert_metric = evaluate.load('bertscore')\n","\n","def compute_metrics(preds_and_labels):\n","  # preds are not logits but token ids\n","    # api is inconsistent here\n","    # we are not simply using argmax bu use 'beam search'\n","    preds, labels = preds_and_labels\n","\n","    # convert predictions into words\n","    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","\n","    # for any -100 label, replace with pad token id\n","    labels = np.where( labels != -100, labels, tokenizer.pad_token_id )\n","\n","    # convert labels into words\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens= True)\n","\n","    # get rid of extra whitespace\n","    # and also, put targets into lists\n","\n","    decoded_preds_cleaned = [pred.strip() for pred in decoded_preds]\n","    decoded_labels_cleaned = [label.strip() for label in decoded_labels]\n","\n","    bleu_score = bleu_metric.compute(predictions=decoded_preds_cleaned, references=decoded_labels_cleaned)\n","    bert_score = bert_metric.compute(predictions=decoded_preds_cleaned, references=decoded_labels_cleaned, lang='fr')\n","\n","    return{'bleu_score:': bleu_score['score'], 'bert_score': np.mean(bert_score['f1'])}\n","    # return {'bleu_score:': bleu_score['score']}"],"metadata":{"id":"b6FwIYuLMElV","executionInfo":{"status":"ok","timestamp":1698855832611,"user_tz":300,"elapsed":3820,"user":{"displayName":"Phanindra Josh","userId":"16090189446352440714"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["# **Function to set Trainer**"],"metadata":{"id":"EP5b8-6VMm8r"}},{"cell_type":"code","source":["def get_trainer(model, training_args, tokenized_dataset, compute_metrics, tokenizer, data_collator):\n","  # initialize trainer\n","  trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_dataset[\"val\"],\n","    compute_metrics=compute_metrics,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n",")\n","  return trainer\n"],"metadata":{"id":"ArEXn-BRMo9O","executionInfo":{"status":"ok","timestamp":1698855832611,"user_tz":300,"elapsed":6,"user":{"displayName":"Phanindra Josh","userId":"16090189446352440714"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["# **Function to free memory**"],"metadata":{"id":"8hfmWqpTN7fB"}},{"cell_type":"code","source":["def free_memory():\n","    \"\"\"\n","    Attempts to free up memory by deleting variables and running Python's garbage collector.\n","    \"\"\"\n","    gc.collect()\n","    for device_id in range(torch.cuda.device_count()):\n","        torch.cuda.set_device(device_id)\n","        torch.cuda.empty_cache()\n","    gc.collect()"],"metadata":{"id":"PDFw0_gbN787","executionInfo":{"status":"ok","timestamp":1698855832611,"user_tz":300,"elapsed":4,"user":{"displayName":"Phanindra Josh","userId":"16090189446352440714"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["# **Function to tokenize dataset and, train and eval models**"],"metadata":{"id":"Q4OdsVTiNmUq"}},{"cell_type":"code","source":["def tokenize_train_evaluate_log(training_args, checkpoint, base_folder, max_length,\n","                                train_val_subset, compute_metrics):\n","    # 1. Free memory\n","    free_memory()\n","\n","    # 2. Setup wandb\n","    wandb.login()\n","    %env WANDB_PROJECT = nlp_course_fall_2023-HW6-Part-D-Colab\n","\n","\n","    # 3. Get Tokenized Dataset and Data Collator\n","    train_val_tokenized_dataset = get_tokenized_dataset(checkpoint, train_val_subset,max_length)\n","\n","    # 4. Initialize Model and Tokenizer\n","    model = initialize_model(checkpoint)\n","    global tokenizer\n","    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","    # 5. Initialize Trainer\n","    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer,model=model)\n","    trainer = get_trainer(model, training_args, train_val_tokenized_dataset,\n","                          compute_metrics, tokenizer, data_collator)\n","\n","    # 6. Train and Evaluate\n","    trainer.train()\n","    trainer.evaluate(train_val_tokenized_dataset['val'])\n","\n","\n","    best_model_checkpoint_step = trainer.state.best_model_checkpoint.split('-')[-1]\n","    wandb.log({\"best_model_checkpoint_step\": best_model_checkpoint_step})\n","    print(f\"The best model was saved at step {best_model_checkpoint_step}.\")\n","\n","    wandb.finish()"],"metadata":{"id":"1-rpu1rHNsGN","executionInfo":{"status":"ok","timestamp":1698855832611,"user_tz":300,"elapsed":4,"user":{"displayName":"Phanindra Josh","userId":"16090189446352440714"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["# **Initial Training Arguments**"],"metadata":{"id":"avT0jrZjPDqy"}},{"cell_type":"code","source":["def training_args_fn(checkpoint,base_folder):\n","  # Configure training parameters\n","\n","  # Define the directory where model checkpoints will be saved\n","  model_folder = base_folder / \"Models\" / \"nlp_fall_2023/kde4/opus-mt-en-fr\"\n","\n","  # Create the directory if it doesn't exist\n","  model_folder.mkdir(exist_ok=True, parents=True)\n","\n","  training_args = Seq2SeqTrainingArguments(\n","      # Training-specific configurations\n","      num_train_epochs=1,  # Total number of training epochs\n","      weight_decay=0.01,  # Apply L2 regularization to prevent overfitting\n","      learning_rate=5e-5,  # Step size for the optimizer during training\n","      optim=\"adamw_torch\",  # Optimizer,\n","      warmup_steps=10,\n","      predict_with_generate=True,\n","      generation_config=GenerationConfig.from_pretrained(checkpoint),\n","      # memory and speed related arguments\n","      # Number of samples per training batch for each device\n","      per_device_train_batch_size=16,\n","      per_device_eval_batch_size=16,  # Number of samples per eval batch for each device\n","\n","      gradient_checkpointing=True,  # memory\n","      # fp16 = True, # Speed\n","      # bf16=True,\n","      # tf32=True, # speed\n","      # evaluation settings\n","      output_dir=str(model_folder),  # Directory to save model checkpoints\n","      evaluation_strategy=\"steps\",  # Evaluate model at specified step intervals\n","      eval_steps=10,  # Perform evaluation every 10 training steps\n","      # Checkpoint settings\n","      save_strategy=\"steps\",  # Save model checkpoint at specified step intervals\n","      save_steps=10,  # Save a model checkpoint every 10 training steps\n","      load_best_model_at_end=True,  # Reload the best model at the end of training\n","      save_total_limit=2,  # Retain only the best and the most recent model checkpoints\n","      # metric_for_best_model=,\n","      # greater_is_better=,\n","      # Experiment logging configurations (commented out in this example)\n","      logging_strategy=\"steps\",\n","      logging_steps=10,\n","      report_to=\"wandb\",  # Log metrics and results to Weights & Biases platform\n","      # Experiment name for Weights & Biases\n","      run_name=\"translation-exp1\",\n","  )\n","  return training_args"],"metadata":{"id":"2_5w29bhPKGC","executionInfo":{"status":"ok","timestamp":1698855832611,"user_tz":300,"elapsed":4,"user":{"displayName":"Phanindra Josh","userId":"16090189446352440714"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["# **Experiments**"],"metadata":{"id":"sHNSesuXR20K"}},{"cell_type":"markdown","source":["# **Dataset hyperparameters**"],"metadata":{"id":"336nE6PLSfy3"}},{"cell_type":"code","source":["name_of_the_datacard = 'kde4'\n","lang1 = 'en'\n","lang2 = 'fr'\n","train_val_subset, train_val_dataset, test_dataset = setup_dataset(name_of_the_datacard,lang1,lang2)"],"metadata":{"id":"LbVvLodcR86x","executionInfo":{"status":"ok","timestamp":1698855835539,"user_tz":300,"elapsed":2932,"user":{"displayName":"Phanindra Josh","userId":"16090189446352440714"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["# **Experiment 1 : with model --> Helsinki-NLP/opus-mt-en-fr and Learning rate = 5e-5**"],"metadata":{"id":"IylAYeJnTt4H"}},{"cell_type":"markdown","source":["# **Trainer hyperparameters**"],"metadata":{"id":"5JU579QQT-Qr"}},{"cell_type":"code","source":["checkpoint = 'Helsinki-NLP/opus-mt-en-fr'\n","exp1 = 'helsinki-model'\n","max_length = 128\n","training_args = training_args_fn(checkpoint, base_folder)\n","training_args_dict = training_args.to_dict() # Convert TrainingArguments to dictionary\n","\n","training_args_dict['run_name'] = f'{checkpoint}-{exp1}' # Update the run_name\n","new_training_args = Seq2SeqTrainingArguments(**training_args_dict)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mt2vz-aiUBQi","executionInfo":{"status":"ok","timestamp":1698855836111,"user_tz":300,"elapsed":579,"user":{"displayName":"Phanindra Josh","userId":"16090189446352440714"}},"outputId":"5a5da280-ac9d-45da-d42a-679dd316c15f"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1711: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["tokenize_train_evaluate_log(training_args= training_args,\n","                            checkpoint=checkpoint, base_folder=base_folder, max_length = max_length,\n","                            train_val_subset=train_val_subset,compute_metrics=compute_metrics)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"fXyBsOvgWIEq","executionInfo":{"status":"ok","timestamp":1698856542523,"user_tz":300,"elapsed":706415,"user":{"displayName":"Phanindra Josh","userId":"16090189446352440714"}},"outputId":"569651ef-7f35-4725-fff0-97a25e3c3bb6"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpxa210024\u001b[0m (\u001b[33mpooja_rocks\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"stream","name":"stdout","text":["env: WANDB_PROJECT=nlp_course_fall_2023-HW6-Part-D-Colab\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n","  warnings.warn(\"Recommended: pip install sacremoses.\")\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.15.12"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20231101_162415-7t7b7013</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/pooja_rocks/nlp_course_fall_2023-HW6-Part-D-Colab/runs/7t7b7013' target=\"_blank\">translation-exp1</a></strong> to <a href='https://wandb.ai/pooja_rocks/nlp_course_fall_2023-HW6-Part-D-Colab' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/pooja_rocks/nlp_course_fall_2023-HW6-Part-D-Colab' target=\"_blank\">https://wandb.ai/pooja_rocks/nlp_course_fall_2023-HW6-Part-D-Colab</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/pooja_rocks/nlp_course_fall_2023-HW6-Part-D-Colab/runs/7t7b7013' target=\"_blank\">https://wandb.ai/pooja_rocks/nlp_course_fall_2023-HW6-Part-D-Colab/runs/7t7b7013</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [63/63 09:40, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Bleu Score:</th>\n","      <th>Bert Score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>1.991500</td>\n","      <td>1.655252</td>\n","      <td>39.680601</td>\n","      <td>0.863847</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.733100</td>\n","      <td>1.596057</td>\n","      <td>39.234273</td>\n","      <td>0.863176</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>1.672000</td>\n","      <td>1.556005</td>\n","      <td>40.578745</td>\n","      <td>0.866220</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>1.442300</td>\n","      <td>1.537132</td>\n","      <td>41.174389</td>\n","      <td>0.867369</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>1.439700</td>\n","      <td>1.526197</td>\n","      <td>41.246359</td>\n","      <td>0.867248</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>1.512200</td>\n","      <td>1.520451</td>\n","      <td>41.166549</td>\n","      <td>0.866630</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [32/32 01:24]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["The best model was saved at step 60.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/bert_score</td><td>▂▁▆██▇▇</td></tr><tr><td>eval/bleu_score:</td><td>▃▁▆████</td></tr><tr><td>eval/loss</td><td>█▅▃▂▁▁▁</td></tr><tr><td>eval/runtime</td><td>▂█▁▂▄▅▄</td></tr><tr><td>eval/samples_per_second</td><td>▆▁█▇▄▃▄</td></tr><tr><td>eval/steps_per_second</td><td>▆▁█▇▄▃▄</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▄▄▅▅▆▆████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▄▄▅▅▆▆█████</td></tr><tr><td>train/learning_rate</td><td>█▇▅▄▂▁</td></tr><tr><td>train/loss</td><td>█▅▄▁▁▂</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_model_checkpoint_step</td><td>60</td></tr><tr><td>eval/bert_score</td><td>0.86663</td></tr><tr><td>eval/bleu_score:</td><td>41.16655</td></tr><tr><td>eval/loss</td><td>1.52045</td></tr><tr><td>eval/runtime</td><td>92.2558</td></tr><tr><td>eval/samples_per_second</td><td>5.42</td></tr><tr><td>eval/steps_per_second</td><td>0.347</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>63</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.5122</td></tr><tr><td>train/total_flos</td><td>15072060506112.0</td></tr><tr><td>train/train_loss</td><td>1.62256</td></tr><tr><td>train/train_runtime</td><td>585.579</td></tr><tr><td>train/train_samples_per_second</td><td>1.708</td></tr><tr><td>train/train_steps_per_second</td><td>0.108</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">translation-exp1</strong> at: <a href='https://wandb.ai/pooja_rocks/nlp_course_fall_2023-HW6-Part-D-Colab/runs/7t7b7013' target=\"_blank\">https://wandb.ai/pooja_rocks/nlp_course_fall_2023-HW6-Part-D-Colab/runs/7t7b7013</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20231101_162415-7t7b7013/logs</code>"]},"metadata":{}}]},{"cell_type":"markdown","source":["# **Experiment 2 : with model --> t5-small and Learning rate = 5e-5**"],"metadata":{"id":"UAYLPBeDmY-S"}},{"cell_type":"code","source":["checkpoint = 't5-small'\n","exp1 = 't5-model'\n","max_length = 128\n","training_args = training_args_fn(checkpoint, base_folder)\n","training_args_dict = training_args.to_dict() # Convert TrainingArguments to dictionary\n","\n","training_args_dict['run_name'] = f'{checkpoint}-{exp1}' # Update the run_name\n","new_training_args = Seq2SeqTrainingArguments(**training_args_dict)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AetF49EVmeZf","executionInfo":{"status":"ok","timestamp":1698856543013,"user_tz":300,"elapsed":521,"user":{"displayName":"Phanindra Josh","userId":"16090189446352440714"}},"outputId":"1183c93b-9eb8-4277-accb-644a6667fa9d"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1711: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["tokenize_train_evaluate_log(training_args= training_args,\n","                            checkpoint=checkpoint, base_folder=base_folder, max_length = max_length,\n","                            train_val_subset=train_val_subset,compute_metrics=compute_metrics)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["58c6f171452b4af49dc67f2bfdbc6675","0ec909869aef4028af376a2d42387478","5d708059058440afb1a8ffbf42dd2d82","fbd084f96ade49cb9ec145b4c8946e1b","970e893fd21d40908fe2030aa83ac797","942ec39c7f684257ac40264be13fd72f","403503dd2dc14351a51ff7c55886da0d","8711ef51c2e54ea194f06b51a1cf59ac","8ab725b7aedc492f82dc5692b68b1670","61df353a586a4b94bde09efea7cf53fe","ca7967376896487da87890f47dfe4fd0"]},"id":"x1kk5qr0msKP","executionInfo":{"status":"ok","timestamp":1698856678909,"user_tz":300,"elapsed":135898,"user":{"displayName":"Phanindra Josh","userId":"16090189446352440714"}},"outputId":"6470fe5c-a112-4e5f-9140-6736e91d00bb"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["env: WANDB_PROJECT=nlp_course_fall_2023-HW6-Part-D-Colab\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/500 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58c6f171452b4af49dc67f2bfdbc6675"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.15.12"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20231101_163546-s0rnnzcf</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/pooja_rocks/nlp_course_fall_2023-HW6-Part-D-Colab/runs/s0rnnzcf' target=\"_blank\">translation-exp1</a></strong> to <a href='https://wandb.ai/pooja_rocks/nlp_course_fall_2023-HW6-Part-D-Colab' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/pooja_rocks/nlp_course_fall_2023-HW6-Part-D-Colab' target=\"_blank\">https://wandb.ai/pooja_rocks/nlp_course_fall_2023-HW6-Part-D-Colab</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/pooja_rocks/nlp_course_fall_2023-HW6-Part-D-Colab/runs/s0rnnzcf' target=\"_blank\">https://wandb.ai/pooja_rocks/nlp_course_fall_2023-HW6-Part-D-Colab/runs/s0rnnzcf</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [63/63 01:49, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Bleu Score:</th>\n","      <th>Bert Score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>3.996800</td>\n","      <td>3.170202</td>\n","      <td>3.056221</td>\n","      <td>0.692575</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>3.697600</td>\n","      <td>2.845025</td>\n","      <td>3.648438</td>\n","      <td>0.706263</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>3.360200</td>\n","      <td>2.654505</td>\n","      <td>4.295556</td>\n","      <td>0.710954</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>2.941400</td>\n","      <td>2.566919</td>\n","      <td>5.012549</td>\n","      <td>0.718981</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>2.886100</td>\n","      <td>2.508355</td>\n","      <td>5.495368</td>\n","      <td>0.718796</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>2.789400</td>\n","      <td>2.481039</td>\n","      <td>5.952252</td>\n","      <td>0.725476</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [32/32 00:09]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["The best model was saved at step 60.\n"]},{"output_type":"stream","name":"stderr","text":["Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/bert_score</td><td>▁▄▅▇▇██</td></tr><tr><td>eval/bleu_score:</td><td>▁▂▄▆▇██</td></tr><tr><td>eval/loss</td><td>█▅▃▂▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▇█▄▅▄▆</td></tr><tr><td>eval/samples_per_second</td><td>█▁▁▅▄▄▃</td></tr><tr><td>eval/steps_per_second</td><td>█▁▁▅▄▄▃</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▄▄▅▅▆▆████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▄▄▅▅▆▆█████</td></tr><tr><td>train/learning_rate</td><td>█▇▅▄▂▁</td></tr><tr><td>train/loss</td><td>█▆▄▂▂▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_model_checkpoint_step</td><td>60</td></tr><tr><td>eval/bert_score</td><td>0.72548</td></tr><tr><td>eval/bleu_score:</td><td>5.95225</td></tr><tr><td>eval/loss</td><td>2.48104</td></tr><tr><td>eval/runtime</td><td>11.89</td></tr><tr><td>eval/samples_per_second</td><td>42.052</td></tr><tr><td>eval/steps_per_second</td><td>2.691</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>63</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.7894</td></tr><tr><td>train/total_flos</td><td>15976676720640.0</td></tr><tr><td>train/train_loss</td><td>3.2304</td></tr><tr><td>train/train_runtime</td><td>111.4926</td></tr><tr><td>train/train_samples_per_second</td><td>8.969</td></tr><tr><td>train/train_steps_per_second</td><td>0.565</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">translation-exp1</strong> at: <a href='https://wandb.ai/pooja_rocks/nlp_course_fall_2023-HW6-Part-D-Colab/runs/s0rnnzcf' target=\"_blank\">https://wandb.ai/pooja_rocks/nlp_course_fall_2023-HW6-Part-D-Colab/runs/s0rnnzcf</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20231101_163546-s0rnnzcf/logs</code>"]},"metadata":{}}]},{"cell_type":"markdown","source":["# **Experiment 3 : with model --> Helsinki-NLP/opus-mt-en-fr, Learning rate = 5e-4**"],"metadata":{"id":"_myKLp7Lnsnp"}},{"cell_type":"code","source":["checkpoint = 'Helsinki-NLP/opus-mt-en-fr'\n","exp1 = 'helsinki-model'\n","max_length = 128\n","training_args = training_args_fn(checkpoint, base_folder)\n","training_args_dict = training_args.to_dict() # Convert TrainingArguments to dictionary\n","training_args_dict['learning_rate'] = 5e-4\n","training_args_dict['run_name'] = f'{checkpoint}-{exp1}' # Update the run_name\n","new_training_args = Seq2SeqTrainingArguments(**training_args_dict)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QuFHlHPqn6XX","executionInfo":{"status":"ok","timestamp":1698856679440,"user_tz":300,"elapsed":557,"user":{"displayName":"Phanindra Josh","userId":"16090189446352440714"}},"outputId":"22dba241-af26-4f6c-b1e9-21afc8c09039"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1711: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["tokenize_train_evaluate_log(training_args= training_args,\n","                            checkpoint=checkpoint, base_folder=base_folder, max_length = max_length,\n","                            train_val_subset=train_val_subset,compute_metrics=compute_metrics)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ZqwTTJwaoBxE","executionInfo":{"status":"ok","timestamp":1698857346215,"user_tz":300,"elapsed":666777,"user":{"displayName":"Phanindra Josh","userId":"16090189446352440714"}},"outputId":"bdc03847-8090-4045-82fe-0841b5b11623"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["env: WANDB_PROJECT=nlp_course_fall_2023-HW6-Part-D-Colab\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n","  warnings.warn(\"Recommended: pip install sacremoses.\")\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.15.12"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20231101_163808-91tmv8bf</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/pooja_rocks/nlp_course_fall_2023-HW6-Part-D-Colab/runs/91tmv8bf' target=\"_blank\">translation-exp1</a></strong> to <a href='https://wandb.ai/pooja_rocks/nlp_course_fall_2023-HW6-Part-D-Colab' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/pooja_rocks/nlp_course_fall_2023-HW6-Part-D-Colab' target=\"_blank\">https://wandb.ai/pooja_rocks/nlp_course_fall_2023-HW6-Part-D-Colab</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/pooja_rocks/nlp_course_fall_2023-HW6-Part-D-Colab/runs/91tmv8bf' target=\"_blank\">https://wandb.ai/pooja_rocks/nlp_course_fall_2023-HW6-Part-D-Colab/runs/91tmv8bf</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [63/63 09:13, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Bleu Score:</th>\n","      <th>Bert Score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>1.991500</td>\n","      <td>1.655252</td>\n","      <td>39.680601</td>\n","      <td>0.863847</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.733100</td>\n","      <td>1.596057</td>\n","      <td>39.234273</td>\n","      <td>0.863176</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>1.672000</td>\n","      <td>1.556005</td>\n","      <td>40.578745</td>\n","      <td>0.866220</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>1.442300</td>\n","      <td>1.537132</td>\n","      <td>41.174389</td>\n","      <td>0.867369</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>1.439700</td>\n","      <td>1.526197</td>\n","      <td>41.246359</td>\n","      <td>0.867248</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>1.512200</td>\n","      <td>1.520451</td>\n","      <td>41.166549</td>\n","      <td>0.866630</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [32/32 01:25]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["The best model was saved at step 60.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/bert_score</td><td>▂▁▆██▇▇</td></tr><tr><td>eval/bleu_score:</td><td>▃▁▆████</td></tr><tr><td>eval/loss</td><td>█▅▃▂▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁█▁▂▅▅▅</td></tr><tr><td>eval/samples_per_second</td><td>█▁█▆▃▃▃</td></tr><tr><td>eval/steps_per_second</td><td>█▁█▆▃▃▃</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▄▄▅▅▆▆████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▄▄▅▅▆▆█████</td></tr><tr><td>train/learning_rate</td><td>█▇▅▄▂▁</td></tr><tr><td>train/loss</td><td>█▅▄▁▁▂</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_model_checkpoint_step</td><td>60</td></tr><tr><td>eval/bert_score</td><td>0.86663</td></tr><tr><td>eval/bleu_score:</td><td>41.16655</td></tr><tr><td>eval/loss</td><td>1.52045</td></tr><tr><td>eval/runtime</td><td>93.4791</td></tr><tr><td>eval/samples_per_second</td><td>5.349</td></tr><tr><td>eval/steps_per_second</td><td>0.342</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>63</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.5122</td></tr><tr><td>train/total_flos</td><td>15072060506112.0</td></tr><tr><td>train/train_loss</td><td>1.62256</td></tr><tr><td>train/train_runtime</td><td>555.9156</td></tr><tr><td>train/train_samples_per_second</td><td>1.799</td></tr><tr><td>train/train_steps_per_second</td><td>0.113</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">translation-exp1</strong> at: <a href='https://wandb.ai/pooja_rocks/nlp_course_fall_2023-HW6-Part-D-Colab/runs/91tmv8bf' target=\"_blank\">https://wandb.ai/pooja_rocks/nlp_course_fall_2023-HW6-Part-D-Colab/runs/91tmv8bf</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20231101_163808-91tmv8bf/logs</code>"]},"metadata":{}}]},{"cell_type":"markdown","source":["# **Conclusion:**\n","Compared to helsinki model, t5 model's bert score is relatively less. And for helsinki model changing the learning rate from 5e-5 to 5e-4 didn't make much difference in the model performance."],"metadata":{"id":"RKHQRtTAr4xP"}},{"cell_type":"code","source":["!sudo apt-get install texlive-xetex texlive-fonts-recommended texlive-plain-generic"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ijNvt_CzyhQP","executionInfo":{"status":"ok","timestamp":1698857431589,"user_tz":300,"elapsed":70916,"user":{"displayName":"Phanindra Josh","userId":"16090189446352440714"}},"outputId":"cd3a8da3-f81b-4a17-a148-628dcca3f6bf"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  dvisvgm fonts-droid-fallback fonts-lato fonts-lmodern fonts-noto-mono\n","  fonts-texgyre fonts-urw-base35 libapache-pom-java libcommons-logging-java\n","  libcommons-parent-java libfontbox-java libfontenc1 libgs9 libgs9-common\n","  libidn12 libijs-0.35 libjbig2dec0 libkpathsea6 libpdfbox-java libptexenc1\n","  libruby3.0 libsynctex2 libteckit0 libtexlua53 libtexluajit2 libwoff1\n","  libzzip-0-13 lmodern poppler-data preview-latex-style rake ruby\n","  ruby-net-telnet ruby-rubygems ruby-webrick ruby-xmlrpc ruby3.0\n","  rubygems-integration t1utils teckit tex-common tex-gyre texlive-base\n","  texlive-binaries texlive-latex-base texlive-latex-extra\n","  texlive-latex-recommended texlive-pictures tipa xfonts-encodings\n","  xfonts-utils\n","Suggested packages:\n","  fonts-noto fonts-freefont-otf | fonts-freefont-ttf libavalon-framework-java\n","  libcommons-logging-java-doc libexcalibur-logkit-java liblog4j1.2-java\n","  poppler-utils ghostscript fonts-japanese-mincho | fonts-ipafont-mincho\n","  fonts-japanese-gothic | fonts-ipafont-gothic fonts-arphic-ukai\n","  fonts-arphic-uming fonts-nanum ri ruby-dev bundler debhelper gv\n","  | postscript-viewer perl-tk xpdf | pdf-viewer xzdec\n","  texlive-fonts-recommended-doc texlive-latex-base-doc python3-pygments\n","  icc-profiles libfile-which-perl libspreadsheet-parseexcel-perl\n","  texlive-latex-extra-doc texlive-latex-recommended-doc texlive-luatex\n","  texlive-pstricks dot2tex prerex texlive-pictures-doc vprerex\n","  default-jre-headless tipa-doc\n","The following NEW packages will be installed:\n","  dvisvgm fonts-droid-fallback fonts-lato fonts-lmodern fonts-noto-mono\n","  fonts-texgyre fonts-urw-base35 libapache-pom-java libcommons-logging-java\n","  libcommons-parent-java libfontbox-java libfontenc1 libgs9 libgs9-common\n","  libidn12 libijs-0.35 libjbig2dec0 libkpathsea6 libpdfbox-java libptexenc1\n","  libruby3.0 libsynctex2 libteckit0 libtexlua53 libtexluajit2 libwoff1\n","  libzzip-0-13 lmodern poppler-data preview-latex-style rake ruby\n","  ruby-net-telnet ruby-rubygems ruby-webrick ruby-xmlrpc ruby3.0\n","  rubygems-integration t1utils teckit tex-common tex-gyre texlive-base\n","  texlive-binaries texlive-fonts-recommended texlive-latex-base\n","  texlive-latex-extra texlive-latex-recommended texlive-pictures\n","  texlive-plain-generic texlive-xetex tipa xfonts-encodings xfonts-utils\n","0 upgraded, 54 newly installed, 0 to remove and 19 not upgraded.\n","Need to get 182 MB of archives.\n","After this operation, 571 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-droid-fallback all 1:6.0.1r16-1.1build1 [1,805 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-lato all 2.0-2.1 [2,696 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 poppler-data all 0.4.11-1 [2,171 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tex-common all 6.17 [33.7 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-urw-base35 all 20200910-1 [6,367 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9-common all 9.55.0~dfsg1-0ubuntu5.5 [752 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libidn12 amd64 1.38-4ubuntu1 [60.0 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libijs-0.35 amd64 0.35-15build2 [16.5 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjbig2dec0 amd64 0.19-3build2 [64.7 kB]\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9 amd64 9.55.0~dfsg1-0ubuntu5.5 [5,030 kB]\n","Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libkpathsea6 amd64 2021.20210626.59705-1ubuntu0.1 [60.3 kB]\n","Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwoff1 amd64 1.0.2-1build4 [45.2 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 dvisvgm amd64 2.13.1-1 [1,221 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-lmodern all 2.004.5-6.1 [4,532 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-noto-mono all 20201225-1build1 [397 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-texgyre all 20180621-3.1 [10.2 MB]\n","Get:17 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libapache-pom-java all 18-1 [4,720 B]\n","Get:18 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcommons-parent-java all 43-1 [10.8 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcommons-logging-java all 1.2-2 [60.3 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n","Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libptexenc1 amd64 2021.20210626.59705-1ubuntu0.1 [39.1 kB]\n","Get:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 rubygems-integration all 1.18 [5,336 B]\n","Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ruby3.0 amd64 3.0.2-7ubuntu2.4 [50.1 kB]\n","Get:24 http://archive.ubuntu.com/ubuntu jammy/main amd64 ruby-rubygems all 3.3.5-2 [228 kB]\n","Get:25 http://archive.ubuntu.com/ubuntu jammy/main amd64 ruby amd64 1:3.0~exp1 [5,100 B]\n","Get:26 http://archive.ubuntu.com/ubuntu jammy/main amd64 rake all 13.0.6-2 [61.7 kB]\n","Get:27 http://archive.ubuntu.com/ubuntu jammy/main amd64 ruby-net-telnet all 0.1.1-2 [12.6 kB]\n","Get:28 http://archive.ubuntu.com/ubuntu jammy/universe amd64 ruby-webrick all 1.7.0-3 [51.8 kB]\n","Get:29 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ruby-xmlrpc all 0.3.2-1ubuntu0.1 [24.9 kB]\n","Get:30 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libruby3.0 amd64 3.0.2-7ubuntu2.4 [5,113 kB]\n","Get:31 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libsynctex2 amd64 2021.20210626.59705-1ubuntu0.1 [55.5 kB]\n","Get:32 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libteckit0 amd64 2.5.11+ds1-1 [421 kB]\n","Get:33 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtexlua53 amd64 2021.20210626.59705-1ubuntu0.1 [120 kB]\n","Get:34 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtexluajit2 amd64 2021.20210626.59705-1ubuntu0.1 [267 kB]\n","Get:35 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libzzip-0-13 amd64 0.13.72+dfsg.1-1.1 [27.0 kB]\n","Get:36 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n","Get:37 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n","Get:38 http://archive.ubuntu.com/ubuntu jammy/universe amd64 lmodern all 2.004.5-6.1 [9,471 kB]\n","Get:39 http://archive.ubuntu.com/ubuntu jammy/universe amd64 preview-latex-style all 12.2-1ubuntu1 [185 kB]\n","Get:40 http://archive.ubuntu.com/ubuntu jammy/main amd64 t1utils amd64 1.41-4build2 [61.3 kB]\n","Get:41 http://archive.ubuntu.com/ubuntu jammy/universe amd64 teckit amd64 2.5.11+ds1-1 [699 kB]\n","Get:42 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tex-gyre all 20180621-3.1 [6,209 kB]\n","Get:43 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 texlive-binaries amd64 2021.20210626.59705-1ubuntu0.1 [9,848 kB]\n","Get:44 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-base all 2021.20220204-1 [21.0 MB]\n","Get:45 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-fonts-recommended all 2021.20220204-1 [4,972 kB]\n","Get:46 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-latex-base all 2021.20220204-1 [1,128 kB]\n","Get:47 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libfontbox-java all 1:1.8.16-2 [207 kB]\n","Get:48 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libpdfbox-java all 1:1.8.16-2 [5,199 kB]\n","Get:49 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-latex-recommended all 2021.20220204-1 [14.4 MB]\n","Get:50 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-pictures all 2021.20220204-1 [8,720 kB]\n","Get:51 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-latex-extra all 2021.20220204-1 [13.9 MB]\n","Get:52 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-plain-generic all 2021.20220204-1 [27.5 MB]\n","Get:53 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tipa all 2:1.3-21 [2,967 kB]\n","Get:54 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-xetex all 2021.20220204-1 [12.4 MB]\n","Fetched 182 MB in 17s (10.8 MB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 54.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package fonts-droid-fallback.\n","(Reading database ... 120874 files and directories currently installed.)\n","Preparing to unpack .../00-fonts-droid-fallback_1%3a6.0.1r16-1.1build1_all.deb ...\n","Unpacking fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n","Selecting previously unselected package fonts-lato.\n","Preparing to unpack .../01-fonts-lato_2.0-2.1_all.deb ...\n","Unpacking fonts-lato (2.0-2.1) ...\n","Selecting previously unselected package poppler-data.\n","Preparing to unpack .../02-poppler-data_0.4.11-1_all.deb ...\n","Unpacking poppler-data (0.4.11-1) ...\n","Selecting previously unselected package tex-common.\n","Preparing to unpack .../03-tex-common_6.17_all.deb ...\n","Unpacking tex-common (6.17) ...\n","Selecting previously unselected package fonts-urw-base35.\n","Preparing to unpack .../04-fonts-urw-base35_20200910-1_all.deb ...\n","Unpacking fonts-urw-base35 (20200910-1) ...\n","Selecting previously unselected package libgs9-common.\n","Preparing to unpack .../05-libgs9-common_9.55.0~dfsg1-0ubuntu5.5_all.deb ...\n","Unpacking libgs9-common (9.55.0~dfsg1-0ubuntu5.5) ...\n","Selecting previously unselected package libidn12:amd64.\n","Preparing to unpack .../06-libidn12_1.38-4ubuntu1_amd64.deb ...\n","Unpacking libidn12:amd64 (1.38-4ubuntu1) ...\n","Selecting previously unselected package libijs-0.35:amd64.\n","Preparing to unpack .../07-libijs-0.35_0.35-15build2_amd64.deb ...\n","Unpacking libijs-0.35:amd64 (0.35-15build2) ...\n","Selecting previously unselected package libjbig2dec0:amd64.\n","Preparing to unpack .../08-libjbig2dec0_0.19-3build2_amd64.deb ...\n","Unpacking libjbig2dec0:amd64 (0.19-3build2) ...\n","Selecting previously unselected package libgs9:amd64.\n","Preparing to unpack .../09-libgs9_9.55.0~dfsg1-0ubuntu5.5_amd64.deb ...\n","Unpacking libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.5) ...\n","Selecting previously unselected package libkpathsea6:amd64.\n","Preparing to unpack .../10-libkpathsea6_2021.20210626.59705-1ubuntu0.1_amd64.deb ...\n","Unpacking libkpathsea6:amd64 (2021.20210626.59705-1ubuntu0.1) ...\n","Selecting previously unselected package libwoff1:amd64.\n","Preparing to unpack .../11-libwoff1_1.0.2-1build4_amd64.deb ...\n","Unpacking libwoff1:amd64 (1.0.2-1build4) ...\n","Selecting previously unselected package dvisvgm.\n","Preparing to unpack .../12-dvisvgm_2.13.1-1_amd64.deb ...\n","Unpacking dvisvgm (2.13.1-1) ...\n","Selecting previously unselected package fonts-lmodern.\n","Preparing to unpack .../13-fonts-lmodern_2.004.5-6.1_all.deb ...\n","Unpacking fonts-lmodern (2.004.5-6.1) ...\n","Selecting previously unselected package fonts-noto-mono.\n","Preparing to unpack .../14-fonts-noto-mono_20201225-1build1_all.deb ...\n","Unpacking fonts-noto-mono (20201225-1build1) ...\n","Selecting previously unselected package fonts-texgyre.\n","Preparing to unpack .../15-fonts-texgyre_20180621-3.1_all.deb ...\n","Unpacking fonts-texgyre (20180621-3.1) ...\n","Selecting previously unselected package libapache-pom-java.\n","Preparing to unpack .../16-libapache-pom-java_18-1_all.deb ...\n","Unpacking libapache-pom-java (18-1) ...\n","Selecting previously unselected package libcommons-parent-java.\n","Preparing to unpack .../17-libcommons-parent-java_43-1_all.deb ...\n","Unpacking libcommons-parent-java (43-1) ...\n","Selecting previously unselected package libcommons-logging-java.\n","Preparing to unpack .../18-libcommons-logging-java_1.2-2_all.deb ...\n","Unpacking libcommons-logging-java (1.2-2) ...\n","Selecting previously unselected package libfontenc1:amd64.\n","Preparing to unpack .../19-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n","Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Selecting previously unselected package libptexenc1:amd64.\n","Preparing to unpack .../20-libptexenc1_2021.20210626.59705-1ubuntu0.1_amd64.deb ...\n","Unpacking libptexenc1:amd64 (2021.20210626.59705-1ubuntu0.1) ...\n","Selecting previously unselected package rubygems-integration.\n","Preparing to unpack .../21-rubygems-integration_1.18_all.deb ...\n","Unpacking rubygems-integration (1.18) ...\n","Selecting previously unselected package ruby3.0.\n","Preparing to unpack .../22-ruby3.0_3.0.2-7ubuntu2.4_amd64.deb ...\n","Unpacking ruby3.0 (3.0.2-7ubuntu2.4) ...\n","Selecting previously unselected package ruby-rubygems.\n","Preparing to unpack .../23-ruby-rubygems_3.3.5-2_all.deb ...\n","Unpacking ruby-rubygems (3.3.5-2) ...\n","Selecting previously unselected package ruby.\n","Preparing to unpack .../24-ruby_1%3a3.0~exp1_amd64.deb ...\n","Unpacking ruby (1:3.0~exp1) ...\n","Selecting previously unselected package rake.\n","Preparing to unpack .../25-rake_13.0.6-2_all.deb ...\n","Unpacking rake (13.0.6-2) ...\n","Selecting previously unselected package ruby-net-telnet.\n","Preparing to unpack .../26-ruby-net-telnet_0.1.1-2_all.deb ...\n","Unpacking ruby-net-telnet (0.1.1-2) ...\n","Selecting previously unselected package ruby-webrick.\n","Preparing to unpack .../27-ruby-webrick_1.7.0-3_all.deb ...\n","Unpacking ruby-webrick (1.7.0-3) ...\n","Selecting previously unselected package ruby-xmlrpc.\n","Preparing to unpack .../28-ruby-xmlrpc_0.3.2-1ubuntu0.1_all.deb ...\n","Unpacking ruby-xmlrpc (0.3.2-1ubuntu0.1) ...\n","Selecting previously unselected package libruby3.0:amd64.\n","Preparing to unpack .../29-libruby3.0_3.0.2-7ubuntu2.4_amd64.deb ...\n","Unpacking libruby3.0:amd64 (3.0.2-7ubuntu2.4) ...\n","Selecting previously unselected package libsynctex2:amd64.\n","Preparing to unpack .../30-libsynctex2_2021.20210626.59705-1ubuntu0.1_amd64.deb ...\n","Unpacking libsynctex2:amd64 (2021.20210626.59705-1ubuntu0.1) ...\n","Selecting previously unselected package libteckit0:amd64.\n","Preparing to unpack .../31-libteckit0_2.5.11+ds1-1_amd64.deb ...\n","Unpacking libteckit0:amd64 (2.5.11+ds1-1) ...\n","Selecting previously unselected package libtexlua53:amd64.\n","Preparing to unpack .../32-libtexlua53_2021.20210626.59705-1ubuntu0.1_amd64.deb ...\n","Unpacking libtexlua53:amd64 (2021.20210626.59705-1ubuntu0.1) ...\n","Selecting previously unselected package libtexluajit2:amd64.\n","Preparing to unpack .../33-libtexluajit2_2021.20210626.59705-1ubuntu0.1_amd64.deb ...\n","Unpacking libtexluajit2:amd64 (2021.20210626.59705-1ubuntu0.1) ...\n","Selecting previously unselected package libzzip-0-13:amd64.\n","Preparing to unpack .../34-libzzip-0-13_0.13.72+dfsg.1-1.1_amd64.deb ...\n","Unpacking libzzip-0-13:amd64 (0.13.72+dfsg.1-1.1) ...\n","Selecting previously unselected package xfonts-encodings.\n","Preparing to unpack .../35-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n","Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Selecting previously unselected package xfonts-utils.\n","Preparing to unpack .../36-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n","Unpacking xfonts-utils (1:7.7+6build2) ...\n","Selecting previously unselected package lmodern.\n","Preparing to unpack .../37-lmodern_2.004.5-6.1_all.deb ...\n","Unpacking lmodern (2.004.5-6.1) ...\n","Selecting previously unselected package preview-latex-style.\n","Preparing to unpack .../38-preview-latex-style_12.2-1ubuntu1_all.deb ...\n","Unpacking preview-latex-style (12.2-1ubuntu1) ...\n","Selecting previously unselected package t1utils.\n","Preparing to unpack .../39-t1utils_1.41-4build2_amd64.deb ...\n","Unpacking t1utils (1.41-4build2) ...\n","Selecting previously unselected package teckit.\n","Preparing to unpack .../40-teckit_2.5.11+ds1-1_amd64.deb ...\n","Unpacking teckit (2.5.11+ds1-1) ...\n","Selecting previously unselected package tex-gyre.\n","Preparing to unpack .../41-tex-gyre_20180621-3.1_all.deb ...\n","Unpacking tex-gyre (20180621-3.1) ...\n","Selecting previously unselected package texlive-binaries.\n","Preparing to unpack .../42-texlive-binaries_2021.20210626.59705-1ubuntu0.1_amd64.deb ...\n","Unpacking texlive-binaries (2021.20210626.59705-1ubuntu0.1) ...\n","Selecting previously unselected package texlive-base.\n","Preparing to unpack .../43-texlive-base_2021.20220204-1_all.deb ...\n","Unpacking texlive-base (2021.20220204-1) ...\n","Selecting previously unselected package texlive-fonts-recommended.\n","Preparing to unpack .../44-texlive-fonts-recommended_2021.20220204-1_all.deb ...\n","Unpacking texlive-fonts-recommended (2021.20220204-1) ...\n","Selecting previously unselected package texlive-latex-base.\n","Preparing to unpack .../45-texlive-latex-base_2021.20220204-1_all.deb ...\n","Unpacking texlive-latex-base (2021.20220204-1) ...\n","Selecting previously unselected package libfontbox-java.\n","Preparing to unpack .../46-libfontbox-java_1%3a1.8.16-2_all.deb ...\n","Unpacking libfontbox-java (1:1.8.16-2) ...\n","Selecting previously unselected package libpdfbox-java.\n","Preparing to unpack .../47-libpdfbox-java_1%3a1.8.16-2_all.deb ...\n","Unpacking libpdfbox-java (1:1.8.16-2) ...\n","Selecting previously unselected package texlive-latex-recommended.\n","Preparing to unpack .../48-texlive-latex-recommended_2021.20220204-1_all.deb ...\n","Unpacking texlive-latex-recommended (2021.20220204-1) ...\n","Selecting previously unselected package texlive-pictures.\n","Preparing to unpack .../49-texlive-pictures_2021.20220204-1_all.deb ...\n","Unpacking texlive-pictures (2021.20220204-1) ...\n","Selecting previously unselected package texlive-latex-extra.\n","Preparing to unpack .../50-texlive-latex-extra_2021.20220204-1_all.deb ...\n","Unpacking texlive-latex-extra (2021.20220204-1) ...\n","Selecting previously unselected package texlive-plain-generic.\n","Preparing to unpack .../51-texlive-plain-generic_2021.20220204-1_all.deb ...\n","Unpacking texlive-plain-generic (2021.20220204-1) ...\n","Selecting previously unselected package tipa.\n","Preparing to unpack .../52-tipa_2%3a1.3-21_all.deb ...\n","Unpacking tipa (2:1.3-21) ...\n","Selecting previously unselected package texlive-xetex.\n","Preparing to unpack .../53-texlive-xetex_2021.20220204-1_all.deb ...\n","Unpacking texlive-xetex (2021.20220204-1) ...\n","Setting up fonts-lato (2.0-2.1) ...\n","Setting up fonts-noto-mono (20201225-1build1) ...\n","Setting up libwoff1:amd64 (1.0.2-1build4) ...\n","Setting up libtexlua53:amd64 (2021.20210626.59705-1ubuntu0.1) ...\n","Setting up libijs-0.35:amd64 (0.35-15build2) ...\n","Setting up libtexluajit2:amd64 (2021.20210626.59705-1ubuntu0.1) ...\n","Setting up libfontbox-java (1:1.8.16-2) ...\n","Setting up rubygems-integration (1.18) ...\n","Setting up libzzip-0-13:amd64 (0.13.72+dfsg.1-1.1) ...\n","Setting up fonts-urw-base35 (20200910-1) ...\n","Setting up poppler-data (0.4.11-1) ...\n","Setting up tex-common (6.17) ...\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n","debconf: falling back to frontend: Readline\n","update-language: texlive-base not installed and configured, doing nothing!\n","Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Setting up libjbig2dec0:amd64 (0.19-3build2) ...\n","Setting up libteckit0:amd64 (2.5.11+ds1-1) ...\n","Setting up libapache-pom-java (18-1) ...\n","Setting up ruby-net-telnet (0.1.1-2) ...\n","Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Setting up t1utils (1.41-4build2) ...\n","Setting up libidn12:amd64 (1.38-4ubuntu1) ...\n","Setting up fonts-texgyre (20180621-3.1) ...\n","Setting up libkpathsea6:amd64 (2021.20210626.59705-1ubuntu0.1) ...\n","Setting up ruby-webrick (1.7.0-3) ...\n","Setting up fonts-lmodern (2.004.5-6.1) ...\n","Setting up fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n","Setting up ruby-xmlrpc (0.3.2-1ubuntu0.1) ...\n","Setting up libsynctex2:amd64 (2021.20210626.59705-1ubuntu0.1) ...\n","Setting up libgs9-common (9.55.0~dfsg1-0ubuntu5.5) ...\n","Setting up teckit (2.5.11+ds1-1) ...\n","Setting up libpdfbox-java (1:1.8.16-2) ...\n","Setting up libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.5) ...\n","Setting up preview-latex-style (12.2-1ubuntu1) ...\n","Setting up libcommons-parent-java (43-1) ...\n","Setting up dvisvgm (2.13.1-1) ...\n","Setting up libcommons-logging-java (1.2-2) ...\n","Setting up xfonts-utils (1:7.7+6build2) ...\n","Setting up libptexenc1:amd64 (2021.20210626.59705-1ubuntu0.1) ...\n","Setting up texlive-binaries (2021.20210626.59705-1ubuntu0.1) ...\n","update-alternatives: using /usr/bin/xdvi-xaw to provide /usr/bin/xdvi.bin (xdvi.bin) in auto mode\n","update-alternatives: using /usr/bin/bibtex.original to provide /usr/bin/bibtex (bibtex) in auto mode\n","Setting up lmodern (2.004.5-6.1) ...\n","Setting up texlive-base (2021.20220204-1) ...\n","/usr/bin/ucfr\n","/usr/bin/ucfr\n","/usr/bin/ucfr\n","/usr/bin/ucfr\n","mktexlsr: Updating /var/lib/texmf/ls-R-TEXLIVEDIST... \n","mktexlsr: Updating /var/lib/texmf/ls-R-TEXMFMAIN... \n","mktexlsr: Updating /var/lib/texmf/ls-R... \n","mktexlsr: Done.\n","tl-paper: setting paper size for dvips to a4: /var/lib/texmf/dvips/config/config-paper.ps\n","tl-paper: setting paper size for dvipdfmx to a4: /var/lib/texmf/dvipdfmx/dvipdfmx-paper.cfg\n","tl-paper: setting paper size for xdvi to a4: /var/lib/texmf/xdvi/XDvi-paper\n","tl-paper: setting paper size for pdftex to a4: /var/lib/texmf/tex/generic/tex-ini-files/pdftexconfig.tex\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n","debconf: falling back to frontend: Readline\n","Setting up tex-gyre (20180621-3.1) ...\n","Setting up texlive-plain-generic (2021.20220204-1) ...\n","Setting up texlive-latex-base (2021.20220204-1) ...\n","Setting up texlive-latex-recommended (2021.20220204-1) ...\n","Setting up texlive-pictures (2021.20220204-1) ...\n","Setting up texlive-fonts-recommended (2021.20220204-1) ...\n","Setting up tipa (2:1.3-21) ...\n","Setting up texlive-latex-extra (2021.20220204-1) ...\n","Setting up texlive-xetex (2021.20220204-1) ...\n","Setting up rake (13.0.6-2) ...\n","Setting up libruby3.0:amd64 (3.0.2-7ubuntu2.4) ...\n","Setting up ruby3.0 (3.0.2-7ubuntu2.4) ...\n","Setting up ruby (1:3.0~exp1) ...\n","Setting up ruby-rubygems (3.3.5-2) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","Processing triggers for tex-common (6.17) ...\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n","debconf: falling back to frontend: Readline\n","Running updmap-sys. This may take some time... done.\n","Running mktexlsr /var/lib/texmf ... done.\n","Building format(s) --all.\n","\tThis may take some time... done.\n"]}]},{"cell_type":"code","source":["!jupyter nbconvert --to pdf /content/drive/MyDrive/Pooja_HP_Singh_Projects/NLP/HW6/Pooja_Akkaladevi_HW6d.ipynb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z1aqBhSuy40f","executionInfo":{"status":"ok","timestamp":1698857540065,"user_tz":300,"elapsed":12685,"user":{"displayName":"Phanindra Josh","userId":"16090189446352440714"}},"outputId":"fd4d83fd-fab7-4996-f399-30ea225d16f9"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["[NbConvertApp] Converting notebook /content/drive/MyDrive/Pooja_HP_Singh_Projects/NLP/HW6/Pooja_Akkaladevi_HW6d.ipynb to pdf\n","[NbConvertApp] Writing 116533 bytes to notebook.tex\n","[NbConvertApp] Building PDF\n","[NbConvertApp] Running xelatex 3 times: ['xelatex', 'notebook.tex', '-quiet']\n","[NbConvertApp] Running bibtex 1 time: ['bibtex', 'notebook']\n","[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n","[NbConvertApp] PDF successfully created\n","[NbConvertApp] Writing 93759 bytes to /content/drive/MyDrive/Pooja_HP_Singh_Projects/NLP/HW6/Pooja_Akkaladevi_HW6d.pdf\n"]}]}]}